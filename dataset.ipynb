{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiViewAnomalyDataset(Dataset):\n",
    "#     def __init__(self, root_dir, scene_name, sequence_length=2, transform=None, train=True, max_sequences=5):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.scene_name = scene_name\n",
    "#         self.sequence_length = sequence_length\n",
    "#         self.transform = transform or transforms.Compose([\n",
    "#             transforms.Resize((32, 32)),  # Very small image size\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#         ])\n",
    "#         self.train = train\n",
    "#         self.max_sequences = max_sequences\n",
    "        \n",
    "#         # Cache for processed frames\n",
    "#         self.frame_cache = {}\n",
    "        \n",
    "#         # Load frame paths and labels\n",
    "#         self.view_frames = self._load_view_frames()\n",
    "        \n",
    "#         # Pre-process and cache all frames\n",
    "#         self._preprocess_frames()\n",
    "        \n",
    "#     def _preprocess_frames(self):\n",
    "#         print(f\"Pre-processing frames for {self.scene_name} {'Train' if self.train else 'Test'} set...\")\n",
    "#         for view_name, frames in self.view_frames.items():\n",
    "#             self.frame_cache[view_name] = []\n",
    "#             for frame in frames:\n",
    "#                 if isinstance(frame, str):\n",
    "#                     frame = Image.open(frame).convert('RGB')\n",
    "#                 else:\n",
    "#                     frame = Image.fromarray(frame)\n",
    "                \n",
    "#                 if self.transform:\n",
    "#                     frame = self.transform(frame)\n",
    "#                 self.frame_cache[view_name].append(frame)\n",
    "#             print(f\"Processed {len(self.frame_cache[view_name])} frames for {view_name}\")\n",
    "#     def _load_view_frames(self):\n",
    "#         scene_dir = os.path.join(self.root_dir, self.scene_name)\n",
    "#         view_frames = {}\n",
    "        \n",
    "#         if not os.path.exists(scene_dir):\n",
    "#             print(f\"Warning: Scene directory {scene_dir} not found\")\n",
    "#             return {}\n",
    "        \n",
    "#         # Process available views\n",
    "#         for view_name in ['View_1', 'View_2', 'View_3']:\n",
    "#             view_path = os.path.join(scene_dir, view_name)\n",
    "#             if not os.path.exists(view_path):\n",
    "#                 print(f\"Warning: {view_name} not found in {self.scene_name}\")\n",
    "#                 continue\n",
    "                \n",
    "#             if self.train:\n",
    "#                 frames_dir = os.path.join(view_path, 'Train')\n",
    "#             else:\n",
    "#                 frames_dir = os.path.join(view_path, 'Test')\n",
    "            \n",
    "#             if not os.path.exists(frames_dir):\n",
    "#                 print(f\"Warning: {frames_dir} not found\")\n",
    "#                 continue\n",
    "                \n",
    "#             frame_paths = sorted([\n",
    "#                 os.path.join(frames_dir, f) for f in os.listdir(frames_dir)\n",
    "#                 if f.endswith(('.jpg', '.png', '.avi'))\n",
    "#             ])\n",
    "            \n",
    "#             if frame_paths:\n",
    "#                 if frame_paths[0].endswith('.avi'):\n",
    "#                     frame_paths = self._extract_frames_from_video(frame_paths[0], stride=32)\n",
    "                \n",
    "#                 max_frames = self.max_sequences * self.sequence_length\n",
    "#                 view_frames[view_name] = frame_paths[:max_frames]\n",
    "        \n",
    "#         # Check if we have at least one view\n",
    "#         if not view_frames:\n",
    "#             print(f\"Warning: No valid views found for {self.scene_name}\")\n",
    "#             return {}\n",
    "        \n",
    "#         return view_frames\n",
    "\n",
    "#     def _extract_frames_from_video(self, video_path, stride=32):\n",
    "#         frames = []\n",
    "#         cap = cv2.VideoCapture(video_path)\n",
    "#         frame_count = 0\n",
    "        \n",
    "#         while cap.isOpened() and len(frames) < self.max_sequences * self.sequence_length:\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "            \n",
    "#             if frame_count % stride == 0:\n",
    "#                 frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#                 frames.append(frame)\n",
    "            \n",
    "#             frame_count += 1\n",
    "            \n",
    "#         cap.release()\n",
    "#         return frames\n",
    "#     def __len__(self):\n",
    "#         if not self.view_frames:\n",
    "#             return 0\n",
    "#         min_frames = min(len(frames) for frames in self.view_frames.values())\n",
    "#         return max(0, min_frames - self.sequence_length + 1)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         sequence_tensors = []\n",
    "#         required_views = ['View_1', 'View_2', 'View_3']  # Define required views\n",
    "        \n",
    "#         # Ensure we process views in the same order\n",
    "#         for view_name in required_views:\n",
    "#             if view_name not in self.view_frames:\n",
    "#                 # If view is missing, create a zero tensor of the right size\n",
    "#                 view_tensor = torch.zeros(3, self.sequence_length, 64, 64)\n",
    "#             else:\n",
    "#                 view_sequence = []\n",
    "#                 for i in range(self.sequence_length):\n",
    "#                     frame = self.frame_cache[view_name][idx + i]\n",
    "#                     view_sequence.append(frame)\n",
    "                \n",
    "#                 view_tensor = torch.stack(view_sequence)  # (sequence_length, channels, H, W)\n",
    "#                 view_tensor = view_tensor.permute(1, 0, 2, 3)  # (channels, sequence_length, H, W)\n",
    "            \n",
    "#             sequence_tensors.append(view_tensor)\n",
    "        \n",
    "#         sequence_tensor = torch.stack(sequence_tensors)  # (num_views, channels, time, H, W)\n",
    "#         label = torch.tensor([[0.0]] if self.train else [[1.0]], dtype=torch.float32)\n",
    "        \n",
    "#         return sequence_tensor, label\n",
    "\n",
    "# def get_dataloaders(root_dir, batch_size=2, sequence_length=2, num_workers=0):\n",
    "#     train_datasets = []\n",
    "#     test_datasets = []\n",
    "    \n",
    "#     for scene_name in ['1_Times_Square', '2_Italy']:\n",
    "#         print(f\"\\nProcessing scene: {scene_name}\")\n",
    "        \n",
    "#         # Training data (normal behavior)\n",
    "#         train_dataset = MultiViewAnomalyDataset(\n",
    "#             root_dir=root_dir,\n",
    "#             scene_name=scene_name,\n",
    "#             sequence_length=sequence_length,\n",
    "#             train=True,\n",
    "#             max_sequences=5\n",
    "#         )\n",
    "#         if len(train_dataset) > 0:\n",
    "#             print(f\"Added {len(train_dataset)} training sequences from {scene_name}\")\n",
    "#             train_datasets.append(train_dataset)\n",
    "        \n",
    "#         # Testing data (abnormal behavior)\n",
    "#         test_dataset = MultiViewAnomalyDataset(\n",
    "#             root_dir=root_dir,\n",
    "#             scene_name=scene_name,\n",
    "#             sequence_length=sequence_length,\n",
    "#             train=False,\n",
    "#             max_sequences=5\n",
    "#         )\n",
    "#         if len(test_dataset) > 0:\n",
    "#             print(f\"Added {len(test_dataset)} testing sequences from {scene_name}\")\n",
    "#             test_datasets.append(test_dataset)\n",
    "    \n",
    "#     if not train_datasets and not test_datasets:\n",
    "#         print(\"No valid datasets found!\")\n",
    "#         return None, None\n",
    "    \n",
    "#     # Create data loaders\n",
    "#     train_loader = None\n",
    "#     test_loader = None\n",
    "    \n",
    "#     if train_datasets:\n",
    "#         train_dataset = torch.utils.data.ConcatDataset(train_datasets)\n",
    "#         train_loader = DataLoader(\n",
    "#             train_dataset,\n",
    "#             batch_size=batch_size,\n",
    "#             shuffle=True,\n",
    "#             num_workers=num_workers,\n",
    "#             pin_memory=True\n",
    "#         )\n",
    "#         print(f\"Created training loader with {len(train_dataset)} sequences\")\n",
    "    \n",
    "#     if test_datasets:\n",
    "#         test_dataset = torch.utils.data.ConcatDataset(test_datasets)\n",
    "#         test_loader = DataLoader(\n",
    "#             test_dataset,\n",
    "#             batch_size=batch_size,\n",
    "#             shuffle=False,\n",
    "#             num_workers=num_workers,\n",
    "#             pin_memory=True\n",
    "#         )\n",
    "#         print(f\"Created testing loader with {len(test_dataset)} sequences\")\n",
    "    \n",
    "#     return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class MultiViewAnomalyDataset(Dataset):\n",
    "    def __init__(self, root_dir, scene_name='1_Times_Square', sequence_length=2, transform=None, train=True, max_sequences=5):\n",
    "        self.root_dir = root_dir\n",
    "        self.scene_name = scene_name  # Only using Times Square\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((32, 32)),  # Even smaller size\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.train = train\n",
    "        self.max_sequences = max_sequences\n",
    "        self.frame_cache = {}\n",
    "        \n",
    "        # Only use first 3 views\n",
    "        self.required_views = ['View_1', 'View_2', 'View_3']\n",
    "        self.frames_per_view = {}\n",
    "        \n",
    "        # Load and preprocess frames\n",
    "        self._load_and_preprocess()\n",
    "    \n",
    "    def _load_and_preprocess(self):\n",
    "        scene_dir = os.path.join(self.root_dir, self.scene_name)\n",
    "        \n",
    "        for view_name in self.required_views:\n",
    "            view_path = os.path.join(scene_dir, view_name)\n",
    "            split_dir = 'Train' if self.train else 'Test'\n",
    "            frames_dir = os.path.join(view_path, split_dir)\n",
    "            \n",
    "            if not os.path.exists(frames_dir):\n",
    "                continue\n",
    "                \n",
    "            # Get frame paths\n",
    "            frame_paths = sorted([\n",
    "                os.path.join(frames_dir, f) for f in os.listdir(frames_dir)\n",
    "                if f.endswith(('.jpg', '.png', '.avi'))\n",
    "            ])[:10]  # Only use first 10 frames\n",
    "            \n",
    "            # Process frames\n",
    "            processed_frames = []\n",
    "            for frame_path in frame_paths:\n",
    "                if frame_path.endswith('.avi'):\n",
    "                    img = self._extract_frame_from_video(frame_path)\n",
    "                else:\n",
    "                    img = Image.open(frame_path).convert('RGB')\n",
    "                \n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                processed_frames.append(img)\n",
    "            \n",
    "            if processed_frames:\n",
    "                self.frame_cache[view_name] = processed_frames\n",
    "                self.frames_per_view[view_name] = len(processed_frames)\n",
    "    \n",
    "    def _extract_frame_from_video(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            return Image.fromarray(frame)\n",
    "        return Image.new('RGB', (32, 32), color='black')\n",
    "    \n",
    "    def __len__(self):\n",
    "        if not self.frame_cache:\n",
    "            return 0\n",
    "        min_frames = min(len(frames) for frames in self.frame_cache.values())\n",
    "        return max(0, min_frames - self.sequence_length + 1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence_tensors = []\n",
    "        \n",
    "        for view_name in self.required_views:\n",
    "            if view_name in self.frame_cache:\n",
    "                frames = self.frame_cache[view_name][idx:idx + self.sequence_length]\n",
    "                view_tensor = torch.stack(frames)\n",
    "            else:\n",
    "                # Create dummy tensor if view is missing\n",
    "                view_tensor = torch.zeros(self.sequence_length, 3, 32, 32)\n",
    "            \n",
    "            view_tensor = view_tensor.permute(1, 0, 2, 3)  # (channels, sequence_length, H, W)\n",
    "            sequence_tensors.append(view_tensor)\n",
    "        \n",
    "        sequence_tensor = torch.stack(sequence_tensors)\n",
    "        label = torch.tensor([[0.0]] if self.train else [[1.0]], dtype=torch.float32)\n",
    "        \n",
    "        return sequence_tensor, label\n",
    "\n",
    "def get_dataloaders(root_dir, batch_size=2, sequence_length=2, num_workers=0):\n",
    "    # Only use Times Square scene\n",
    "    train_dataset = MultiViewAnomalyDataset(\n",
    "        root_dir=root_dir,\n",
    "        scene_name='1_Times_Square',\n",
    "        sequence_length=sequence_length,\n",
    "        train=True,\n",
    "        max_sequences=5\n",
    "    )\n",
    "    \n",
    "    test_dataset = MultiViewAnomalyDataset(\n",
    "        root_dir=root_dir,\n",
    "        scene_name='1_Times_Square',\n",
    "        sequence_length=sequence_length,\n",
    "        train=False,\n",
    "        max_sequences=5\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    ) if len(train_dataset) > 0 else None\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    ) if len(test_dataset) > 0 else None\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sign_translate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
