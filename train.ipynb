{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# from model import get_model\n",
    "# from dataset import get_dataloaders\n",
    "# import logging\n",
    "# from datetime import datetime\n",
    "# from tqdm.notebook import tqdm\n",
    "# import torch.cuda.amp as amp\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau  # Add this import\n",
    "\n",
    "# # Setup logging\n",
    "# log_dir = 'logs'\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "# timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# log_file = os.path.join(log_dir, f'training_{timestamp}.log')\n",
    "\n",
    "# logging.basicConfig(\n",
    "#     level=logging.INFO,\n",
    "#     format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "#     handlers=[\n",
    "#         logging.FileHandler(log_file),\n",
    "#         logging.StreamHandler()\n",
    "#     ]\n",
    "# )\n",
    "# logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 14:17:02,442 [INFO] Starting training with configuration: {'data_dir': 'Multi-view high-density anomalous crowd', 'batch_size': 2, 'sequence_length': 2, 'num_epochs': 3, 'learning_rate': 0.001, 'device': 'cpu', 'checkpoint_dir': 'checkpoints', 'max_sequences': 5}\n"
     ]
    }
   ],
   "source": [
    "# # Configuration\n",
    "# config = {\n",
    "#     'data_dir': 'Multi-view high-density anomalous crowd',\n",
    "#     'batch_size': 2,\n",
    "#     'sequence_length': 2,\n",
    "#     'num_epochs': 3,       # Reduced epochs\n",
    "#     'learning_rate': 0.001,\n",
    "#     'device': 'cpu',\n",
    "#     'checkpoint_dir': 'checkpoints',\n",
    "#     'max_sequences': 5\n",
    "# }\n",
    "\n",
    "# # Create checkpoint directory\n",
    "# os.makedirs(config['checkpoint_dir'], exist_ok=True)\n",
    "# logger.info(f'Starting training with configuration: {config}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 14:17:07,049 [INFO] Loading and preprocessing dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing scene: 1_Times_Square\n",
      "Pre-processing frames for 1_Times_Square Train set...\n",
      "Processed 10 frames for View_1\n",
      "Processed 10 frames for View_2\n",
      "Processed 10 frames for View_3\n",
      "Added 9 training sequences from 1_Times_Square\n",
      "Pre-processing frames for 1_Times_Square Test set...\n",
      "Processed 10 frames for View_1\n",
      "Processed 10 frames for View_2\n",
      "Processed 10 frames for View_3\n",
      "Added 9 testing sequences from 1_Times_Square\n",
      "\n",
      "Processing scene: 2_Italy\n",
      "Pre-processing frames for 2_Italy Train set...\n",
      "No valid training sequences found for 2_Italy\n",
      "Pre-processing frames for 2_Italy Test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 14:17:08,941 [INFO] Dataset loaded. Training on 9 sequences\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 frames for View_1\n",
      "Processed 10 frames for View_2\n",
      "Added 9 testing sequences from 2_Italy\n"
     ]
    }
   ],
   "source": [
    "# # Load data\n",
    "# logger.info('Loading and preprocessing dataset...')\n",
    "# train_loader, test_loader = get_dataloaders(\n",
    "#     root_dir=config['data_dir'],\n",
    "#     batch_size=config['batch_size'],\n",
    "#     sequence_length=config['sequence_length'],\n",
    "#     num_workers=0\n",
    "# )\n",
    "\n",
    "# if train_loader is None:\n",
    "#     logger.error('No training data found!')\n",
    "# else:\n",
    "#     logger.info(f'Dataset loaded. Training on {len(train_loader.dataset)} sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 14:17:17,803 [INFO] Initializing model...\n",
      "2025-03-03 14:17:17,847 [INFO] Model initialized\n"
     ]
    }
   ],
   "source": [
    "# # Initialize model\n",
    "# logger.info('Initializing model...')\n",
    "# model = get_model(num_views=3, input_channels=3)\n",
    "# model = model.to(config['device'])\n",
    "# logger.info('Model initialized')\n",
    "\n",
    "# # Initialize optimizer and loss\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=0.01)\n",
    "# criterion = torch.nn.BCELoss()\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)  # Updated this line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "    \n",
    "#     for sequences, labels in tqdm(train_loader, desc='Training'):\n",
    "#         sequences = sequences.to(device)\n",
    "#         labels = labels.to(device).view(-1, 1)  # Reshape labels to match output\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(sequences)\n",
    "#         loss = criterion(outputs, labels)  # Removed squeeze()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     return total_loss / len(train_loader)\n",
    "\n",
    "# def validate(model, val_loader, criterion, device):\n",
    "#     model.eval()\n",
    "#     total_loss = 0\n",
    "#     all_preds = []\n",
    "#     all_labels = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for sequences, labels in tqdm(val_loader, desc='Validation'):\n",
    "#             sequences = sequences.to(device)\n",
    "#             labels = labels.to(device).view(-1, 1)  # Reshape labels to match output\n",
    "            \n",
    "#             outputs = model(sequences)\n",
    "#             loss = criterion(outputs, labels)  # Removed squeeze()\n",
    "            \n",
    "#             total_loss += loss.item()\n",
    "#             all_preds.extend(outputs.cpu().numpy())\n",
    "#             all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "#     return total_loss / len(val_loader), all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 14:17:29,071 [INFO] Starting training loop...\n",
      "2025-03-03 14:17:29,074 [INFO] Epoch 1/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796cdb24d6cf40c598256090832c6263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 14:18:34,926 [INFO] Training Loss: 0.4603\n",
      "2025-03-03 14:18:34,927 [INFO] Running validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9945d3a6b4224c3dacc43b13df8e253f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 14:18:49,420 [ERROR] Error during training: stack expects each tensor to be equal size, but got [3, 3, 2, 64, 64] at entry 0 and [2, 3, 2, 64, 64] at entry 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 3, 2, 64, 64] at entry 0 and [2, 3, 2, 64, 64] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     35\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError during training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     38\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining completed!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[0;32m     14\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRunning validation...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m val_loss, predictions, labels \u001b[38;5;241m=\u001b[39m validate(model, test_loader, criterion, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     16\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Update learning rate\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m, in \u001b[0;36mvalidate\u001b[1;34m(model, val_loader, criterion, device)\u001b[0m\n\u001b[0;32m     23\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sequences, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(val_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     27\u001b[0m         sequences \u001b[38;5;241m=\u001b[39m sequences\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     28\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Reshape labels to match output\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\sign_translate\\Lib\\site-packages\\tqdm\\notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\sign_translate\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\sign_translate\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\sign_translate\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\sign_translate\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\sign_translate\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:317\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\sign_translate\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:174\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    171\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\sign_translate\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32md:\\anaconda\\envs\\sign_translate\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:214\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    212\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    213\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 214\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 3, 2, 64, 64] at entry 0 and [2, 3, 2, 64, 64] at entry 1"
     ]
    }
   ],
   "source": [
    "# # Training loop\n",
    "# best_val_loss = float('inf')\n",
    "# logger.info('Starting training loop...')\n",
    "\n",
    "# for epoch in range(config['num_epochs']):\n",
    "#     logger.info(f'Epoch {epoch+1}/{config[\"num_epochs\"]}')\n",
    "    \n",
    "#     try:\n",
    "#         # Train\n",
    "#         train_loss = train_epoch(model, train_loader, optimizer, criterion, config['device'])\n",
    "#         logger.info(f'Training Loss: {train_loss:.4f}')\n",
    "        \n",
    "#         # Validate\n",
    "#         logger.info('Running validation...')\n",
    "#         val_loss, predictions, labels = validate(model, test_loader, criterion, config['device'])\n",
    "#         logger.info(f'Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "#         # Update learning rate\n",
    "#         scheduler.step(val_loss)\n",
    "        \n",
    "#         # Save best model\n",
    "#         if val_loss < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "#             checkpoint_path = os.path.join(config['checkpoint_dir'], 'best_model.pth')\n",
    "#             torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'best_val_loss': best_val_loss,\n",
    "#                 'config': config\n",
    "#             }, checkpoint_path)\n",
    "#             logger.info(f'Saved best model checkpoint to {checkpoint_path}')\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         logger.error(f'Error during training: {str(e)}')\n",
    "#         raise e\n",
    "\n",
    "# logger.info('Training completed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 14:21:40,411 [INFO] Starting training with configuration: {'data_dir': 'Multi-view high-density anomalous crowd', 'batch_size': 2, 'sequence_length': 2, 'num_epochs': 2, 'learning_rate': 0.001, 'device': 'cpu', 'checkpoint_dir': 'checkpoints'}\n",
      "2025-03-03 14:21:40,413 [INFO] Loading dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing scene: 1_Times_Square\n",
      "Pre-processing frames for 1_Times_Square Train set...\n",
      "Processed 10 frames for View_1\n",
      "Processed 10 frames for View_2\n",
      "Processed 10 frames for View_3\n",
      "Added 9 training sequences from 1_Times_Square\n",
      "Pre-processing frames for 1_Times_Square Test set...\n",
      "Processed 10 frames for View_1\n",
      "Processed 10 frames for View_2\n",
      "Processed 10 frames for View_3\n",
      "Added 9 testing sequences from 1_Times_Square\n",
      "\n",
      "Processing scene: 2_Italy\n",
      "Pre-processing frames for 2_Italy Train set...\n",
      "No valid training sequences found for 2_Italy\n",
      "Pre-processing frames for 2_Italy Test set...\n",
      "Processed 10 frames for View_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 14:21:42,372 [INFO] Dataset loaded. Training on 9 sequences\n",
      "2025-03-03 14:21:42,373 [INFO] Initializing model...\n",
      "2025-03-03 14:21:42,403 [INFO] Model initialized\n",
      "2025-03-03 14:21:42,405 [INFO] Starting training loop...\n",
      "2025-03-03 14:21:42,406 [INFO] Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 frames for View_2\n",
      "Added 9 testing sequences from 2_Italy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d67987a7f7441f69ec94634ca3d27e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([2, 1, 1])) that is different to the input size (torch.Size([2, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 79\u001b[0m\n\u001b[0;32m     77\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     78\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(sequences)\n\u001b[1;32m---> 79\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     80\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     81\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\anaconda\\envs\\sign_translate\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\sign_translate\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\sign_translate\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:621\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mbinary_cross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\sign_translate\\Lib\\site-packages\\torch\\nn\\functional.py:3163\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3161\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3164\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3166\u001b[0m     )\n\u001b[0;32m   3168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3169\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([2, 1, 1])) that is different to the input size (torch.Size([2, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 14:35:06,809 [INFO] Starting training with configuration: {'data_dir': 'Multi-view high-density anomalous crowd', 'batch_size': 2, 'sequence_length': 2, 'num_epochs': 2, 'learning_rate': 0.001, 'device': 'cpu', 'checkpoint_dir': 'checkpoints'}\n",
      "2025-03-03 14:35:06,809 [INFO] Loading dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing scene: 1_Times_Square\n",
      "Pre-processing frames for 1_Times_Square Train set...\n",
      "Processed 10 frames for View_1\n",
      "Processed 10 frames for View_2\n",
      "Processed 10 frames for View_3\n",
      "Added 9 training sequences from 1_Times_Square\n",
      "Pre-processing frames for 1_Times_Square Test set...\n",
      "Processed 10 frames for View_1\n",
      "Processed 10 frames for View_2\n",
      "Processed 10 frames for View_3\n",
      "Added 9 testing sequences from 1_Times_Square\n",
      "\n",
      "Processing scene: 2_Italy\n",
      "Pre-processing frames for 2_Italy Train set...\n",
      "No valid training sequences found for 2_Italy\n",
      "Pre-processing frames for 2_Italy Test set...\n",
      "Processed 10 frames for View_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 14:35:08,812 [INFO] Dataset loaded. Training on 9 sequences\n",
      "2025-03-03 14:35:08,813 [INFO] Initializing model...\n",
      "2025-03-03 14:35:08,843 [INFO] Model initialized\n",
      "2025-03-03 14:35:08,845 [INFO] Starting training loop...\n",
      "2025-03-03 14:35:08,846 [INFO] \n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 frames for View_2\n",
      "Added 9 testing sequences from 2_Italy\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6960e4cd8efc4fe8a978bf7afdf17714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 14:35:24,189 [INFO] Batch 1 Loss: 0.7505\n",
      "2025-03-03 14:35:38,214 [INFO] Batch 2 Loss: 0.6760\n",
      "2025-03-03 14:35:52,613 [INFO] Batch 3 Loss: 0.5373\n",
      "2025-03-03 14:36:06,406 [INFO] Batch 4 Loss: 0.4271\n",
      "2025-03-03 14:36:14,605 [INFO] Batch 5 Loss: 0.3752\n",
      "2025-03-03 14:36:14,607 [INFO] Epoch 1 Average Loss: 0.5532\n",
      "2025-03-03 14:36:14,675 [INFO] New best loss: 0.5532, saved model to checkpoints\\best_model.pth\n",
      "2025-03-03 14:36:14,737 [INFO] Saved checkpoint to checkpoints\\model_epoch_1.pth\n",
      "2025-03-03 14:36:14,739 [INFO] \n",
      "Epoch 2/2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055070d850594f6fa7284aaa9a58d187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-03 14:36:29,717 [INFO] Batch 1 Loss: 0.2574\n",
      "2025-03-03 14:36:46,069 [INFO] Batch 2 Loss: 0.1564\n",
      "2025-03-03 14:37:00,887 [INFO] Batch 3 Loss: 0.1261\n",
      "2025-03-03 14:37:16,838 [INFO] Batch 4 Loss: 0.1109\n",
      "2025-03-03 14:37:25,882 [INFO] Batch 5 Loss: 0.0448\n",
      "2025-03-03 14:37:25,885 [INFO] Epoch 2 Average Loss: 0.1391\n",
      "2025-03-03 14:37:25,952 [INFO] New best loss: 0.1391, saved model to checkpoints\\best_model.pth\n",
      "2025-03-03 14:37:26,023 [INFO] Saved checkpoint to checkpoints\\model_epoch_2.pth\n",
      "2025-03-03 14:37:26,025 [INFO] \n",
      "Training completed!\n",
      "2025-03-03 14:37:26,026 [INFO] Best loss achieved: 0.1391\n",
      "2025-03-03 14:37:26,298 [INFO] Saved final model to checkpoints\\final_model.pth\n",
      "2025-03-03 14:37:26,299 [INFO] \n",
      "Training Summary:\n",
      "2025-03-03 14:37:26,300 [INFO] --------------------------------------------------\n",
      "2025-03-03 14:37:26,301 [INFO] Total epochs: 2\n",
      "2025-03-03 14:37:26,303 [INFO] Initial loss: 0.5532\n",
      "2025-03-03 14:37:26,304 [INFO] Final loss: 0.1391\n",
      "2025-03-03 14:37:26,305 [INFO] Best loss: 0.1391\n",
      "2025-03-03 14:37:26,305 [INFO] Loss improvement: 74.85%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from model import get_model\n",
    "from dataset import get_dataloaders\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setup logging\n",
    "log_dir = 'logs'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "log_file = os.path.join(log_dir, f'training_{timestamp}.log')\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'data_dir': 'Multi-view high-density anomalous crowd',\n",
    "    'batch_size': 2,\n",
    "    'sequence_length': 2,\n",
    "    'num_epochs': 2,\n",
    "    'learning_rate': 0.001,\n",
    "    'device': 'cpu',\n",
    "    'checkpoint_dir': 'checkpoints'\n",
    "}\n",
    "\n",
    "# Create checkpoint directory\n",
    "os.makedirs(config['checkpoint_dir'], exist_ok=True)\n",
    "logger.info(f'Starting training with configuration: {config}')\n",
    "\n",
    "# Load only training data\n",
    "logger.info('Loading dataset...')\n",
    "train_loader, _ = get_dataloaders(\n",
    "    root_dir=config['data_dir'],\n",
    "    batch_size=config['batch_size'],\n",
    "    sequence_length=config['sequence_length'],\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "if train_loader is None:\n",
    "    logger.error('No training data found!')\n",
    "else:\n",
    "    logger.info(f'Dataset loaded. Training on {len(train_loader.dataset)} sequences')\n",
    "    \n",
    "    # Initialize model\n",
    "    logger.info('Initializing model...')\n",
    "    model = get_model(num_views=3, input_channels=3)\n",
    "    model = model.to(config['device'])\n",
    "    logger.info('Model initialized')\n",
    "\n",
    "    # Initialize optimizer and loss\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'])\n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    # Training loop\n",
    "    logger.info('Starting training loop...')\n",
    "    losses = []\n",
    "    batch_losses = []\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        logger.info(f'\\nEpoch {epoch+1}/{config[\"num_epochs\"]}')\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for sequences, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
    "            sequences = sequences.to(config['device'])\n",
    "            labels = labels.to(config['device']).squeeze()  # Fix label shape\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences).squeeze()  # Fix output shape\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            current_loss = loss.item()\n",
    "            epoch_loss += current_loss\n",
    "            batch_count += 1\n",
    "            batch_losses.append(current_loss)\n",
    "            \n",
    "            # Log batch progress\n",
    "            logger.info(f'Batch {batch_count} Loss: {current_loss:.4f}')\n",
    "            \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        logger.info(f'Epoch {epoch+1} Average Loss: {avg_loss:.4f}')\n",
    "        \n",
    "        # Save if best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            best_model_path = os.path.join(config['checkpoint_dir'], 'best_model.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss,\n",
    "                'config': config\n",
    "            }, best_model_path)\n",
    "            logger.info(f'New best loss: {best_loss:.4f}, saved model to {best_model_path}')\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_path = os.path.join(config['checkpoint_dir'], f'model_epoch_{epoch+1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'config': config\n",
    "        }, checkpoint_path)\n",
    "        logger.info(f'Saved checkpoint to {checkpoint_path}')\n",
    "\n",
    "    logger.info('\\nTraining completed!')\n",
    "    logger.info(f'Best loss achieved: {best_loss:.4f}')\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot epoch losses\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, len(losses) + 1), losses, 'b-o')\n",
    "    plt.title('Average Loss per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot batch losses\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(batch_losses, 'r-')\n",
    "    plt.title('Loss per Batch')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_loss.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save final model with complete information\n",
    "    final_model_path = os.path.join(config['checkpoint_dir'], 'final_model.pth')\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': config,\n",
    "        'final_loss': losses[-1],\n",
    "        'best_loss': best_loss,\n",
    "        'all_losses': losses,\n",
    "        'batch_losses': batch_losses,\n",
    "        'training_completed': True\n",
    "    }, final_model_path)\n",
    "    logger.info(f'Saved final model to {final_model_path}')\n",
    "    \n",
    "    # Print final summary\n",
    "    logger.info('\\nTraining Summary:')\n",
    "    logger.info('-' * 50)\n",
    "    logger.info(f'Total epochs: {config[\"num_epochs\"]}')\n",
    "    logger.info(f'Initial loss: {losses[0]:.4f}')\n",
    "    logger.info(f'Final loss: {losses[-1]:.4f}')\n",
    "    logger.info(f'Best loss: {best_loss:.4f}')\n",
    "    logger.info(f'Loss improvement: {((losses[0] - losses[-1])/losses[0])*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sign_translate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
